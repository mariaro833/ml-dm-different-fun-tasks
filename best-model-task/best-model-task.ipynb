{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e77a6d6-2c91-450c-88ca-a0744b2c6395",
   "metadata": {},
   "source": [
    "### Data:\n",
    "* Number of records: 214\n",
    "* Number of features: 9 + 1 (target feature)\n",
    "* Repository URL: https://archive.ics.uci.edu/dataset/42/glass+identification\n",
    " \n",
    "##### Problems:\n",
    "a. Find the best two models by creating a complete pipeline per each model, that explores both models and parameters. Comment and compare the results.\\\n",
    "b. Benchmark the best two models in __a.__ by different cross-validation techniques (at least 3). Comment results.\\\n",
    "c. Run one AutoML calculation on the dataset. How do these results compare with the obtained in __a.__? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1af7de-353b-411c-bee2-30311e87bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a64aa5-47ce-4495-a513-6387f6caf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e809a-5700-479f-bcf0-836a99c7cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_identification = fetch_ucirepo(id=42) \n",
    "X = glass_identification.data.features \n",
    "y = glass_identification.data.targets\n",
    "\n",
    "print(sns.heatmap(X.corr().round(2), annot=True, cmap='coolwarm', center=0))\n",
    "\n",
    "print(\"\\n1. DATASET EXPLORATION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"\\nFeatures:\\n{X.columns.tolist()}\")\n",
    "print(f\"\\nTarget variable:\\n{y.columns.tolist()}\\n\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nFeature statistics:\\n{X.describe()}\")\n",
    "print(f\"\\nMissing values:\\n{X.isnull().sum()}\")\n",
    "\n",
    "y = y.values.ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"2. MODEL 1: RANDOM FOREST CLASSIFIER\")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200, 300],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter grid:\")\n",
    "for param, values in rf_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(\"\\nPerforming GridSearchCV (this may take a moment)...\")\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline, \n",
    "    rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Random Forest Results ---\")\n",
    "print(f\"Best parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "y_pred_rf = rf_best_model.predict(X_test)\n",
    "rf_test_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {rf_test_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "feature_importance_rf = rf_best_model.named_steps['rf'].feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance_rf\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Top 5):\")\n",
    "print(feature_importance_df_rf.head())\n",
    "\n",
    "print(\"3. MODEL 2: GRADIENT BOOSTING CLASSIFIER\")\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "gb_param_grid = {\n",
    "    'gb__n_estimators': [50, 100, 200],\n",
    "    'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb__max_depth': [3, 5, 7],\n",
    "    'gb__min_samples_split': [2, 5, 10],\n",
    "    'gb__min_samples_leaf': [1, 2, 4],\n",
    "    'gb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter grid:\")\n",
    "for param, values in gb_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "print(\"\\nPerforming GridSearchCV (this may take a moment)...\")\n",
    "gb_grid_search = GridSearchCV(\n",
    "    gb_pipeline, \n",
    "    gb_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Gradient Boosting Results ---\")\n",
    "print(f\"Best parameters: {gb_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {gb_grid_search.best_score_:.4f}\")\n",
    "\n",
    "gb_best_model = gb_grid_search.best_estimator_\n",
    "y_pred_gb = gb_best_model.predict(X_test)\n",
    "gb_test_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {gb_test_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_gb = gb_best_model.named_steps['gb'].feature_importances_\n",
    "feature_importance_df_gb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importance_gb\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Top 5):\")\n",
    "print(feature_importance_df_gb.head())\n",
    "\n",
    "print(\"4. COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting'],\n",
    "    'Best CV Accuracy': [rf_grid_search.best_score_, gb_grid_search.best_score_],\n",
    "    'Test Accuracy': [rf_test_accuracy, gb_test_accuracy],\n",
    "    'CV-Test Gap': [rf_grid_search.best_score_ - rf_test_accuracy, \n",
    "                    gb_grid_search.best_score_ - gb_test_accuracy]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"DETAILED ANALYSIS AND COMMENTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Determine winner\n",
    "if rf_test_accuracy > gb_test_accuracy:\n",
    "    winner = \"Random Forest\"\n",
    "    winner_accuracy = rf_test_accuracy\n",
    "    diff = rf_test_accuracy - gb_test_accuracy\n",
    "else:\n",
    "    winner = \"Gradient Boosting\"\n",
    "    winner_accuracy = gb_test_accuracy\n",
    "    diff = gb_test_accuracy - rf_test_accuracy\n",
    "\n",
    "print(f\"\\n✓ WINNER: {winner}\")\n",
    "print(f\"  - Test Accuracy: {winner_accuracy:.4f}\")\n",
    "print(f\"  - Advantage: {diff:.4f} ({diff*100:.2f}%) over the other model\")\n",
    "\n",
    "print(f\"\\n1. PERFORMANCE ANALYSIS:\")\n",
    "print(f\"   - Random Forest achieved {rf_test_accuracy:.2%} test accuracy\")\n",
    "print(f\"   - Gradient Boosting achieved {gb_test_accuracy:.2%} test accuracy\")\n",
    "print(f\"   - Both models show {'good' if min(rf_test_accuracy, gb_test_accuracy) > 0.70 else 'moderate'} performance\")\n",
    "\n",
    "print(f\"\\n2. GENERALIZATION:\")\n",
    "rf_gap = rf_grid_search.best_score_ - rf_test_accuracy\n",
    "gb_gap = gb_grid_search.best_score_ - gb_test_accuracy\n",
    "print(f\"   - Random Forest CV-Test gap: {rf_gap:.4f} ({'overfitting' if rf_gap > 0.05 else 'good generalization'})\")\n",
    "print(f\"   - Gradient Boosting CV-Test gap: {gb_gap:.4f} ({'overfitting' if gb_gap > 0.05 else 'good generalization'})\")\n",
    "\n",
    "print(f\"\\n3. MODEL CHARACTERISTICS:\")\n",
    "print(f\"   Random Forest:\")\n",
    "print(f\"   - Ensemble of decision trees with bagging\")\n",
    "print(f\"   - Best params: {rf_grid_search.best_params_}\")\n",
    "print(f\"   - More robust to outliers and less prone to overfitting\")\n",
    "print(f\"   - Faster training with parallel processing\")\n",
    "print(f\"\\n   Gradient Boosting:\")\n",
    "print(f\"   - Sequential ensemble with boosting\")\n",
    "print(f\"   - Best params: {gb_grid_search.best_params_}\")\n",
    "print(f\"   - Better at capturing complex patterns\")\n",
    "print(f\"   - More sensitive to hyperparameters\")\n",
    "\n",
    "print(f\"\\n4. FEATURE IMPORTANCE COMPARISON:\")\n",
    "print(f\"\\n   Top 3 features - Random Forest:\")\n",
    "for idx, row in feature_importance_df_rf.head(3).iterrows():\n",
    "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
    "print(f\"\\n   Top 3 features - Gradient Boosting:\")\n",
    "for idx, row in feature_importance_df_gb.head(3).iterrows():\n",
    "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS:\")\n",
    "if winner == \"Random Forest\":\n",
    "    print(f\"   ✓ Use Random Forest for deployment\")\n",
    "    print(f\"   - Higher test accuracy\")\n",
    "    print(f\"   - Faster prediction times\")\n",
    "    print(f\"   - More interpretable and stable\")\n",
    "else:\n",
    "    print(f\"   ✓ Use Gradient Boosting for deployment\")\n",
    "    print(f\"   - Higher test accuracy\")\n",
    "    print(f\"   - Better at handling complex relationships\")\n",
    "    print(f\"   - Consider ensemble of both models for production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
