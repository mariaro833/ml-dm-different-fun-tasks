{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d20b1df-7012-4dc4-9b86-17a53417be3b",
   "metadata": {},
   "source": [
    "### Tips for Analysis\n",
    "\n",
    "Stemming: Use when speed matters and approximate matches are okay\n",
    "\n",
    "Lemmatization: Use when accuracy matters and you need real words\n",
    "\n",
    "Stopword removal: Almost always beneficial for content analysis\n",
    "\n",
    "Lowercase: Almost always needed for proper word counting\n",
    "\n",
    "Punctuation: Usually removed, but keep if analyzing sentence structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f27f04-497c-4357-ae4f-3a57af38deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')           # Tokenizer models\n",
    "# nltk.download('stopwords')       # Common word lists\n",
    "# nltk.download('wordnet')         # Lemmatization database\n",
    "# nltk.download('omw-1.4')         # Multilingual WordNet\n",
    "# nltk.download('twitter_samples') # Sample tweets\n",
    "# nltk.download('treebank')        # Parsed corpus\n",
    "# nltk.download('opinion_lexicon') # Sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03737f15-02d3-40b9-8dc6-c2ab513169e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe38dba-f7a8-4844-84df-d5fd726f7126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n"
     ]
    }
   ],
   "source": [
    "RESOURCE_NAME = 'THE PROJECT GUTENBERG EBOOK FRANKENSTEIN'\n",
    "URL = \"https://gutenberg.org/cache/epub/84/pg84.txt\"\n",
    "\n",
    "text = requests.get(URL).text\n",
    "lines = text.splitlines()\n",
    "\n",
    "print(lines[0])\n",
    "print(lines[1])\n",
    "print(lines[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb422562-12eb-4754-b399-dbeee49d4258",
   "metadata": {},
   "source": [
    "#### Clean Text\n",
    "\n",
    "[^\\w\\s]+ removes non-word, non-space characters\n",
    "\n",
    "\\s+ replaces multiple spaces with single space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ffed7-4ac1-430f-be49-f90757f9d31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\?\\!]\", \" \", text)  # keep . ? !\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()           # normalize spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324aa914-afe7-417f-87a6-dd8d9c114626",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "Splits text into words, converts to lowercase, keeps only letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdf9892-a745-4ade-b63e-c6b7aa0acf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "tokens = [token.lower() for token in tokens if token.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b19a67-c9be-4140-aa10-8714f32dbb9e",
   "metadata": {},
   "source": [
    "#### Initial Frequency Distribution\n",
    "Counts word frequencies BEFORE stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c69f92-8ddc-422b-b774-9c34be325e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4387, 'and': 3043, 'i': 2845, 'of': 2764, 'to': 2176, 'my': 1776, 'a': 1448, 'in': 1188, 'that': 1031, 'was': 1019, ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = nltk.FreqDist(tokens)\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faccda9-b109-405e-a274-c0f8a2a2b7f5",
   "metadata": {},
   "source": [
    "#### Remove Stopwords\n",
    "Filters out common words like \"the\", \"is\", \"at\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21241e75-e091-4af8-be79-744e1d53bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "ctokens = [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda83ee6-c8a3-482b-b100-b67cd95ecf65",
   "metadata": {},
   "source": [
    "#### Frequency After Stopword Removal\n",
    "Counts word frequencies AFTER removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f846f5a9-5acf-4547-9ce5-14b1a5425c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq2 = nltk.FreqDist(ctokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c477f5-1b47-400d-a0b5-a83af22bab56",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Reduces words to their root form (crude method)\n",
    "Examples: \"running\" â†’ \"run\", \"studies\" â†’ \"studi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcb6f2a-06c7-465f-850b-425ae08aac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in ctokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3828cb-a1e4-47f0-aa50-820d57c0e246",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "Reduces words to dictionary form (smart method)\n",
    "Examples: \"running\" â†’ \"run\", \"better\" â†’ \"good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a566ced-fd71-4633-b94f-5b1eb767d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in ctokens]\n",
    "# ctokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582589a8-8482-4c5e-9a17-54bde1a47e99",
   "metadata": {},
   "source": [
    "#### Sentiment-analyysi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8b86ca5-7752-40ec-bd91-6b622974a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83f3668-862a-4a38-8549-cdf8fcf81d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens analyzed: 33,314\n",
      "Positive words: 2,834 (8.5%)\n",
      "Negative words: 3,351 (10.1%)\n",
      "Neutral words:  27,129 (81.4%)\n"
     ]
    }
   ],
   "source": [
    "countpositive = countnegative = countneutral = counttotal = 0\n",
    "\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "for token1 in ctokens:\n",
    "    counttotal = counttotal+1\n",
    "    cat = \"\"\n",
    "    if token1 in positive_words:\n",
    "        countpositive = countpositive +1 \n",
    "        cat = cat + \"POS\"\n",
    "    elif token1 in negative_words:\n",
    "        countnegative = countnegative +1 \n",
    "        cat = cat + \"NEG\" \n",
    "    else:\n",
    "        countneutral = countneutral+1\n",
    "\n",
    "print(f\"Total tokens analyzed: {counttotal:,}\")\n",
    "print(f\"Positive words: {countpositive:,} ({countpositive/counttotal*100:.1f}%)\")\n",
    "print(f\"Negative words: {countnegative:,} ({countnegative/counttotal*100:.1f}%)\")\n",
    "print(f\"Neutral words:  {countneutral:,} ({countneutral/counttotal*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b573527b-9843-4f30-b9b1-cac87c944113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall sentiment score: -0.016\n",
      "ðŸ˜ Overall sentiment: NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "sentiment_score = (countpositive - countnegative) / counttotal\n",
    "print(f\"\\nOverall sentiment score: {sentiment_score:.3f}\")\n",
    "if sentiment_score > 0.05:\n",
    "    print(\"ðŸ“ˆ Overall sentiment: POSITIVE\")\n",
    "elif sentiment_score < -0.05:\n",
    "    print(\"ðŸ“‰ Overall sentiment: NEGATIVE\")\n",
    "else:\n",
    "    print(\"ðŸ˜ Overall sentiment: NEUTRAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30611b75-ac58-4cc7-847c-a199b0bcc162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Frankenstein', 'Or', 'The', 'Modern', 'Prometheus']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens3 = word_tokenize(text)\n",
    "print(tokens3[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9972eb22-34ce-4363-adfb-1b433cd36dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4080, 'and': 3003, '.': 2992, 'I': 2849, 'of': 2750, 'to': 2157, 'my': 1635, 'a': 1402, 'in': 1139, 'was': 1020, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq3 = nltk.FreqDist(tokens3)\n",
    "freq3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "482b7e34-362d-43af-9ea0-c1e87acbfb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/mariaro833/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10092cd9-d65d-49c6-ac92-4556b38568d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens3)\n",
    "# tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e5327f-219b-4b59-b02a-0f4a6033fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/mariaro833/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/mariaro833/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mariaro833/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !python -m pip install svgling\n",
    "# You need these for NER:\n",
    "nltk.download('maxent_ne_chunker')      # Named entity chunker\n",
    "nltk.download('words')                   # Word list corpus\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger (if not already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44cb682b-c346-4616-8df9-8b86c3a6d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Frankenstein', 'Or', 'The', 'Modern', 'Prometheus']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens3 = word_tokenize(text)\n",
    "print(tokens3[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e606c6-67be-45d0-a948-aa00fc4bd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities = nltk.chunk.ne_chunk(tagged)\n",
    "# entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182edc9f-1edc-4c3a-80b5-ac6c7081de1b",
   "metadata": {},
   "source": [
    "#### Frequency After Lemmatization\n",
    "Final word frequency count on clean, normalized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed0cb02e-2c3e-4c9c-835e-9dfa8743be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq3 = nltk.FreqDist(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0ce7f-d9a9-48a6-a8a4-be4d8bf580e6",
   "metadata": {},
   "source": [
    "#### Comparative Analysis\n",
    "Compare stemming vs lemmatization\n",
    "This shows which approach is better for different use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "527ac71b-b3ed-47c3-a7c0-4884cc8529f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Stemming vs Lemmatization:\n",
      "================================================================================\n",
      "Original Token       Stemmed              Lemmatized          \n",
      "================================================================================\n",
      "project              project              project             \n",
      "gutenberg            gutenberg            gutenberg           \n",
      "ebook                ebook                ebook               \n",
      "frankenstein         frankenstein         frankenstein        \n",
      "modern               modern               modern              \n",
      "prometheus           prometheu            prometheus          \n",
      "ebook                ebook                ebook               \n",
      "use                  use                  use                 \n",
      "anyone               anyon                anyone              \n",
      "anywhere             anywher              anywhere            \n",
      "united               unit                 united              \n",
      "states               state                state               \n",
      "parts                part                 part                \n",
      "world                world                world               \n",
      "cost                 cost                 cost                \n",
      "almost               almost               almost              \n",
      "restrictions         restrict             restriction         \n",
      "may                  may                  may                 \n",
      "copy                 copi                 copy                \n",
      "give                 give                 give                \n",
      "away                 away                 away                \n",
      "use                  use                  use                 \n",
      "terms                term                 term                \n",
      "project              project              project             \n",
      "gutenberg            gutenberg            gutenberg           \n",
      "license              licens               license             \n",
      "included             includ               included            \n",
      "ebook                ebook                ebook               \n",
      "online               onlin                online              \n",
      "located              locat                located             \n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "  Stemming:      Fast but crude, may create non-words\n",
      "  Lemmatization: Slower but accurate, always produces real words\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing Stemming vs Lemmatization:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Original Token':<20} {'Stemmed':<20} {'Lemmatized':<20}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# first 30 tokens to compare\n",
    "for i in range(min(30, len(ctokens))):\n",
    "    print(f\"{ctokens[i]:<20} {stemmed_tokens[i]:<20} {lemmatized_tokens[i]:<20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary:\")\n",
    "print(f\"  Stemming:      Fast but crude, may create non-words\")\n",
    "print(f\"  Lemmatization: Slower but accurate, always produces real words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b934523e-ea37-4c5d-bca9-83108bd1f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result for future analyses\n",
    "# with open('processed_speech.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(\" \".join(tokens_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7e57b-debe-4860-9ec3-4c3afd005051",
   "metadata": {},
   "source": [
    "#### Keyword analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a05f32-933b-4b97-b286-a38ec5315b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mariaro833/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mariaro833/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a3ec7c3-4785-4ec6-8964-2b24277ec23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEYWORD THEME ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Creation & Science:\n",
      "--------------------------------------------------\n",
      "  create          :   8 times\n",
      "  creation        :  18 times\n",
      "  science         :  23 times\n",
      "  scientific      :   3 times\n",
      "  experiment      :   2 times\n",
      "  life            :  99 times\n",
      "  animate         :   2 times\n",
      "  laboratory      :   3 times\n",
      "  electricity     :   1 times\n",
      "  TOTAL           : 159 mentions\n",
      "\n",
      "Isolation & Loneliness:\n",
      "--------------------------------------------------\n",
      "  alone           :  33 times\n",
      "  lonely          :   2 times\n",
      "  solitude        :  11 times\n",
      "  isolation       :   0 times\n",
      "  desolate        :   5 times\n",
      "  abandoned       :   3 times\n",
      "  secluded        :   3 times\n",
      "  TOTAL           :  57 mentions\n",
      "\n",
      "Nature vs Nurture:\n",
      "--------------------------------------------------\n",
      "  nature          :  49 times\n",
      "  nurture         :   0 times\n",
      "  education       :   5 times\n",
      "  learn           :  11 times\n",
      "  instinct        :   1 times\n",
      "  affection       :  42 times\n",
      "  kindness        :  21 times\n",
      "  TOTAL           : 129 mentions\n",
      "\n",
      "Revenge & Violence:\n",
      "--------------------------------------------------\n",
      "  revenge         :  19 times\n",
      "  murder          :  19 times\n",
      "  death           :  61 times\n",
      "  kill            :   2 times\n",
      "  destroy         :  22 times\n",
      "  vengeance       :  18 times\n",
      "  rage            :  22 times\n",
      "  wrath           :   0 times\n",
      "  TOTAL           : 163 mentions\n",
      "\n",
      "Ambition & Hubris:\n",
      "--------------------------------------------------\n",
      "  ambition        :   6 times\n",
      "  glory           :   7 times\n",
      "  fame            :   1 times\n",
      "  knowledge       :  24 times\n",
      "  power           :  50 times\n",
      "  aspire          :   1 times\n",
      "  genius          :   6 times\n",
      "  TOTAL           :  95 mentions\n",
      "\n",
      "Responsibility & Ethics:\n",
      "--------------------------------------------------\n",
      "  responsibility  :   0 times\n",
      "  duty            :  22 times\n",
      "  guilt           :   7 times\n",
      "  conscience      :   6 times\n",
      "  remorse         :  12 times\n",
      "  consequence     :   5 times\n",
      "  TOTAL           :  52 mentions\n",
      "\n",
      "Humanity & The Monster:\n",
      "--------------------------------------------------\n",
      "  monster         :  29 times\n",
      "  creature        :  60 times\n",
      "  fiend           :  33 times\n",
      "  being           :  15 times\n",
      "  human           :  58 times\n",
      "  wretch          :  17 times\n",
      "  daemon          :   0 times\n",
      "  TOTAL           : 212 mentions\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "keywords = {\n",
    "    'Creation & Science': [\n",
    "        'create', 'creation', 'science', 'scientific', 'experiment',\n",
    "        'life', 'animate', 'laboratory', 'electricity'\n",
    "    ],\n",
    "    'Isolation & Loneliness': [\n",
    "        'alone', 'lonely', 'solitude', 'isolation', 'desolate',\n",
    "        'abandoned', 'secluded'\n",
    "    ],\n",
    "    'Nature vs Nurture': [\n",
    "        'nature', 'nurture', 'education', 'learn', 'instinct',\n",
    "        'affection', 'kindness'\n",
    "    ],\n",
    "    'Revenge & Violence': [\n",
    "        'revenge', 'murder', 'death', 'kill', 'destroy',\n",
    "        'vengeance', 'rage', 'wrath'\n",
    "    ],\n",
    "    'Ambition & Hubris': [\n",
    "        'ambition', 'glory', 'fame', 'knowledge', 'power',\n",
    "        'aspire', 'genius'\n",
    "    ],\n",
    "    'Responsibility & Ethics': [\n",
    "        'responsibility', 'duty', 'guilt', 'conscience',\n",
    "        'remorse', 'consequence'\n",
    "    ],\n",
    "    'Humanity & The Monster': [\n",
    "        'monster', 'creature', 'fiend', 'being', 'human',\n",
    "        'wretch', 'daemon'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEYWORD THEME ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for theme, words in keywords.items():\n",
    "    print(f\"\\n{theme}:\")\n",
    "    print(\"-\" * 50)\n",
    "    theme_total = 0\n",
    "    for word in words:\n",
    "        count = lemmatized_tokens.count(word)\n",
    "        theme_total += count\n",
    "        print(f\"  {word:15s} : {count:3d} times\")\n",
    "    print(f\"  {'TOTAL':15s} : {theme_total:3d} mentions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c74e7ce0-0ac6-467e-9a4c-2cf7e4fa2b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEYWORD THEME ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Visuals:\n",
      "--------------------------------------------------\n",
      "  light           :  40 times\n",
      "  shadow          :   8 times\n",
      "  color           :   0 times\n",
      "  composition     :   0 times\n",
      "  camera          :   0 times\n",
      "  frame           :  10 times\n",
      "  angle           :   0 times\n",
      "  movement        :   0 times\n",
      "  TOTAL           :  58 mentions\n",
      "\n",
      "Sound:\n",
      "--------------------------------------------------\n",
      "  music           :   4 times\n",
      "  silence         :  12 times\n",
      "  dialogue        :   1 times\n",
      "  noise           :   1 times\n",
      "  sfx             :   0 times\n",
      "  voiceover       :   0 times\n",
      "  TOTAL           :  18 mentions\n",
      "\n",
      "Character:\n",
      "--------------------------------------------------\n",
      "  emotion         :   9 times\n",
      "  expression      :  15 times\n",
      "  gesture         :   5 times\n",
      "  reaction        :   0 times\n",
      "  conflict        :   1 times\n",
      "  TOTAL           :  30 mentions\n",
      "\n",
      "Story:\n",
      "--------------------------------------------------\n",
      "  tension         :   0 times\n",
      "  climax          :   0 times\n",
      "  resolution      :   8 times\n",
      "  pacing          :   1 times\n",
      "  turning         :   7 times\n",
      "  reveal          :   2 times\n",
      "  TOTAL           :  18 mentions\n",
      "\n",
      "Mood & Tone:\n",
      "--------------------------------------------------\n",
      "  dark            :  28 times\n",
      "  dramatic        :   0 times\n",
      "  mysterious      :   3 times\n",
      "  sad             :   8 times\n",
      "  tense           :   0 times\n",
      "  joyful          :   1 times\n",
      "  hopeful         :   0 times\n",
      "  TOTAL           :  40 mentions\n",
      "\n",
      "Themes:\n",
      "--------------------------------------------------\n",
      "  identity        :   0 times\n",
      "  isolation       :   0 times\n",
      "  love            :  54 times\n",
      "  loss            :   3 times\n",
      "  memory          :  16 times\n",
      "  technology      :   0 times\n",
      "  freedom         :   2 times\n",
      "  TOTAL           :  75 mentions\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "scene_keywords = {\n",
    "    'Visuals': [\n",
    "        'light', 'shadow', 'color', 'composition', 'camera', 'frame', 'angle', 'movement'\n",
    "    ],\n",
    "\n",
    "    'Sound': [\n",
    "        'music', 'silence', 'dialogue', 'noise', 'sfx', 'voiceover'\n",
    "    ],\n",
    "\n",
    "    'Character': [\n",
    "        'emotion', 'expression', 'gesture', 'reaction', 'conflict'\n",
    "    ],\n",
    "\n",
    "    'Story': [\n",
    "        'tension', 'climax', 'resolution', 'pacing', 'turning', 'reveal'\n",
    "    ],\n",
    "\n",
    "    'Mood & Tone': [\n",
    "        'dark', 'dramatic', 'mysterious', 'sad', 'tense', 'joyful', 'hopeful'\n",
    "    ],\n",
    "\n",
    "    'Themes': [\n",
    "        'identity', 'isolation', 'love', 'loss', 'memory', 'technology', 'freedom'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEYWORD THEME ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for theme, words in scene_keywords.items():\n",
    "    print(f\"\\n{theme}:\")\n",
    "    print(\"-\" * 50)\n",
    "    theme_total = 0\n",
    "    for word in words:\n",
    "        count = lemmatized_tokens.count(word)\n",
    "        theme_total += count\n",
    "        print(f\"  {word:15s} : {count:3d} times\")\n",
    "    print(f\"  {'TOTAL':15s} : {theme_total:3d} mentions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac9e95f-25d4-4204-925b-c37ed8d4de22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TEXT STATISTICS FOR THE PROJECT GUTENBERG EBOOK FRANKENSTEIN\n",
      "======================================================================\n",
      "Total characters:              430,137\n",
      "Total sentences:               3,451\n",
      "Total words (raw):             74,847\n",
      "Content words (no stopwords):  33,314\n",
      "Unique content words:          6,241\n",
      "Lexical diversity:             18.73%\n",
      "Average words per sentence:    21.7\n",
      "Average content words/sentence: 9.7\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "unique_words = len(set(lemmatized_tokens))\n",
    "total_words = len(lemmatized_tokens)\n",
    "\n",
    "# Lexical diversity = unique words / total words\n",
    "# Higher = more varied vocabulary\n",
    "# Lower = more repetitive\n",
    "lexical_diversity = unique_words / total_words\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"TEXT STATISTICS FOR {RESOURCE_NAME}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total characters:              {len(text):,}\")\n",
    "print(f\"Total sentences:               {len(sentences):,}\")\n",
    "print(f\"Total words (raw):             {len(tokens):,}\")\n",
    "print(f\"Content words (no stopwords):  {total_words:,}\")\n",
    "print(f\"Unique content words:          {unique_words:,}\")\n",
    "print(f\"Lexical diversity:             {lexical_diversity:.2%}\")\n",
    "print(f\"Average words per sentence:    {len(tokens)/len(sentences):.1f}\")\n",
    "print(f\"Average content words/sentence: {total_words/len(sentences):.1f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a94eb-67ef-460f-8a7f-cc8e3c860d79",
   "metadata": {},
   "source": [
    "#### Twitter Samples Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a72c649-a591-440b-9a84-7d4af6a798b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "622f0fe0-261f-4415-ab2b-b43aec5b4e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tokens: 38,960\n",
      "Negative tokens: 40,361\n"
     ]
    }
   ],
   "source": [
    "# Combine all positive tweets into one big text and tokenize\n",
    "all_pos_text = ' '.join(positive_tweets).lower()\n",
    "pos_tokens = [word for word in all_pos_text.split() if word.isalpha()]\n",
    "\n",
    "# Combine all negative tweets into one big text and tokenize\n",
    "all_neg_text = ' '.join(negative_tweets).lower()\n",
    "neg_tokens = [word for word in all_neg_text.split() if word.isalpha()]\n",
    "\n",
    "print(f\"Positive tokens: {len(pos_tokens):,}\")\n",
    "print(f\"Negative tokens: {len(neg_tokens):,}\")\n",
    "\n",
    "# Step 3: Create frequency distributions\n",
    "pos_freq = nltk.FreqDist(pos_tokens)\n",
    "neg_freq = nltk.FreqDist(neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d12d444-7848-483c-86ad-c9e88432def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TOP 20 WORDS IN POSITIVE TWEETS:\n",
      "======================================================================\n",
      "you                  :  1,319\n",
      "to                   :  1,083\n",
      "the                  :  1,079\n",
      "i                    :  1,064\n",
      "a                    :    924\n",
      "for                  :    769\n",
      "and                  :    694\n",
      "my                   :    552\n",
      "in                   :    500\n",
      "have                 :    432\n",
      "is                   :    421\n",
      "of                   :    410\n",
      "it                   :    405\n",
      "thanks               :    351\n",
      "your                 :    331\n",
      "on                   :    319\n",
      "me                   :    311\n",
      "follow               :    292\n",
      "this                 :    281\n",
      "so                   :    277\n",
      "\n",
      "======================================================================\n",
      "TOP 20 WORDS IN NEGATIVE TWEETS:\n",
      "======================================================================\n",
      "i                    :  2,184\n",
      "to                   :  1,091\n",
      "the                  :    915\n",
      "my                   :    739\n",
      "and                  :    713\n",
      "you                  :    666\n",
      "a                    :    651\n",
      "me                   :    629\n",
      "so                   :    573\n",
      "is                   :    469\n",
      "it                   :    449\n",
      "but                  :    427\n",
      "in                   :    426\n",
      "for                  :    404\n",
      "of                   :    368\n",
      "have                 :    327\n",
      "this                 :    298\n",
      "on                   :    295\n",
      "that                 :    293\n",
      "not                  :    291\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 20 WORDS IN POSITIVE TWEETS:\")\n",
    "print(\"=\"*70)\n",
    "for word, count in pos_freq.most_common(20):\n",
    "    print(f\"{word:20s} : {count:6,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 20 WORDS IN NEGATIVE TWEETS:\")\n",
    "print(\"=\"*70)\n",
    "for word, count in neg_freq.most_common(20):\n",
    "    print(f\"{word:20s} : {count:6,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea15c8b-0694-485b-880c-beb836f7455e",
   "metadata": {},
   "source": [
    "#### NLTK-polun tulostaminen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1c9347c-1aeb-4aea-816a-7043e544c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.data.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
